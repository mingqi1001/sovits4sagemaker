{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHaw6hGEa_Nk",
    "tags": []
   },
   "source": [
    "# **Initialize environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0gQcIZ8RsOkn",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 29 08:30:06 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A10G                    On  | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   17C    P8              15W / 300W |      2MiB / 23028MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#@title Connect to colab runtime and check GPU\n",
    "\n",
    "#@markdown # Connect to colab runtime and check GPU\n",
    "\n",
    "#@markdown\n",
    "\n",
    "#需成功执行一次，\n",
    "\n",
    "!nvidia-smi\n",
    "!sudo cp  /home/ec2-user/anaconda3/envs/pytorch_p310/lib/libstdc++.so.6.0.32  /usr/lib64/\n",
    "!sudo ln -sf /usr/lib64/libstdc++.so.6.0.32  /usr/lib64/libstdc++.so.6\n",
    "\n",
    "#如果目录存在偏差请执行如下两个命令进行对其查找\n",
    "# ls -l /usr/lib64/libstdc++.so*\n",
    "# sudo find / -name libstdc++.so.6*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0YUGpYrXhMck",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'so-vits-svc' already exists and is not an empty directory.\n",
      "/home/ec2-user/SageMaker/so-vits-svc\n",
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (23.3.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (68.2.2)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-69.0.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading setuptools-69.0.2-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.2.2\n",
      "    Uninstalling setuptools-68.2.2:\n",
      "      Successfully uninstalled setuptools-68.2.2\n",
      "Successfully installed setuptools-69.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
      "Collecting ffmpeg-python (from -r requirements.txt (line 1))\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: Flask in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: Flask_Cors in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.0.0)\n",
      "Collecting gradio>=3.7.0 (from -r requirements.txt (line 4))\n",
      "  Downloading gradio-4.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting numpy==1.23.5 (from -r requirements.txt (line 5))\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyworld (from -r requirements.txt (line 6))\n",
      "  Downloading pyworld-0.3.4.tar.gz (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.0/252.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy==1.10.0 (from -r requirements.txt (line 7))\n",
      "  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting SoundFile==0.12.1 (from -r requirements.txt (line 8))\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch (from -r requirements.txt (line 9))\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2325.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hCollecting torchaudio (from -r requirements.txt (line 10))\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchcrepe (from -r requirements.txt (line 11))\n",
      "  Downloading torchcrepe-0.0.22-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.66.1)\n",
      "Requirement already satisfied: rich in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (13.6.0)\n",
      "Collecting loguru (from -r requirements.txt (line 14))\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting scikit-maad (from -r requirements.txt (line 15))\n",
      "  Downloading scikit_maad-1.4.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting praat-parselmouth (from -r requirements.txt (line 16))\n",
      "  Downloading praat_parselmouth-0.4.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting onnx (from -r requirements.txt (line 17))\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting onnxsim (from -r requirements.txt (line 18))\n",
      "  Downloading onnxsim-0.4.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting onnxoptimizer (from -r requirements.txt (line 19))\n",
      "  Downloading onnxoptimizer-0.3.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (678 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m678.1/678.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fairseq==0.12.2 (from -r requirements.txt (line 20))\n",
      "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting librosa==0.9.1 (from -r requirements.txt (line 21))\n",
      "  Downloading librosa-0.9.1-py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (2.13.0)\n",
      "Collecting tensorboardX (from -r requirements.txt (line 23))\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting transformers (from -r requirements.txt (line 24))\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting edge_tts (from -r requirements.txt (line 25))\n",
      "  Downloading edge_tts-6.1.9-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langdetect (from -r requirements.txt (line 26))\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (6.0.1)\n",
      "Requirement already satisfied: pynvml in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (11.5.0)\n",
      "Collecting faiss-cpu (from -r requirements.txt (line 29))\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops (from -r requirements.txt (line 30))\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting local_attention (from -r requirements.txt (line 31))\n",
      "  Downloading local_attention-1.9.0-py3-none-any.whl.metadata (682 bytes)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from SoundFile==0.12.1->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: cython in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fairseq==0.12.2->-r requirements.txt (line 20)) (3.0.4)\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2->-r requirements.txt (line 20))\n",
      "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2->-r requirements.txt (line 20))\n",
      "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fairseq==0.12.2->-r requirements.txt (line 20)) (2023.10.3)\n",
      "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2->-r requirements.txt (line 20))\n",
      "  Downloading sacrebleu-2.3.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: bitarray in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from fairseq==0.12.2->-r requirements.txt (line 20)) (2.8.2)\n",
      "Collecting audioread>=2.1.5 (from librosa==0.9.1->-r requirements.txt (line 21))\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 21)) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 21)) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 21)) (5.1.1)\n",
      "Collecting resampy>=0.2.2 (from librosa==0.9.1->-r requirements.txt (line 21))\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numba>=0.45.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 21)) (0.57.1)\n",
      "Collecting pooch>=1.0 (from librosa==0.9.1->-r requirements.txt (line 21))\n",
      "  Downloading pooch-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from librosa==0.9.1->-r requirements.txt (line 21)) (21.3)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from ffmpeg-python->-r requirements.txt (line 1)) (0.18.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask->-r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from Flask->-r requirements.txt (line 2)) (1.6.3)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: ffmpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (0.3.0)\n",
      "Collecting gradio-client==0.7.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting huggingface-hub>=0.14.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (3.8.0)\n",
      "Collecting orjson~=3.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (1.5.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (10.0.1)\n",
      "Collecting pydantic>=2.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydub in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (0.25.1)\n",
      "Collecting python-multipart (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests~=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (2.31.0)\n",
      "Collecting semantic-version~=2.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio>=3.7.0->-r requirements.txt (line 4)) (4.5.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=3.7.0->-r requirements.txt (line 4)) (2023.10.0)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (3.12.4)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from torch->-r requirements.txt (line 9)) (3.2)\n",
      "Collecting triton==2.1.0 (from torch->-r requirements.txt (line 9))\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich->-r requirements.txt (line 13)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich->-r requirements.txt (line 13)) (2.16.1)\n",
      "Requirement already satisfied: scikit-image>=0.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-maad->-r requirements.txt (line 15)) (0.22.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from onnx->-r requirements.txt (line 17)) (4.24.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (3.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (69.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (0.7.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 22)) (0.41.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers->-r requirements.txt (line 24))\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 24))\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohttp>=3.8.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from edge_tts->-r requirements.txt (line 25)) (3.8.6)\n",
      "Collecting certifi==2023.07.22 (from edge_tts->-r requirements.txt (line 25))\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from langdetect->-r requirements.txt (line 26)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from aiohttp>=3.8.0->edge_tts->-r requirements.txt (line 25)) (1.3.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (4.19.1)\n",
      "Requirement already satisfied: toolz in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from cffi>=1.0->SoundFile==0.12.1->-r requirements.txt (line 8)) (2.21)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 22)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 22)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 22)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 22)) (1.3.1)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2->-r requirements.txt (line 20))\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 13)) (0.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib~=3.0->gradio>=3.7.0->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib~=3.0->gradio>=3.7.0->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib~=3.0->gradio>=3.7.0->-r requirements.txt (line 4)) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib~=3.0->gradio>=3.7.0->-r requirements.txt (line 4)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib~=3.0->gradio>=3.7.0->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from matplotlib~=3.0->gradio>=3.7.0->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from numba>=0.45.1->librosa==0.9.1->-r requirements.txt (line 21)) (0.40.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio>=3.7.0->-r requirements.txt (line 4)) (2023.3.post1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pooch>=1.0->librosa==0.9.1->-r requirements.txt (line 21)) (3.11.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions~=4.0 (from gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests~=2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests~=2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (1.26.18)\n",
      "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2->-r requirements.txt (line 20))\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2->-r requirements.txt (line 20)) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq==0.12.2->-r requirements.txt (line 20)) (0.4.4)\n",
      "Collecting lxml (from sacrebleu>=1.4.12->fairseq==0.12.2->-r requirements.txt (line 20))\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: imageio>=2.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-image>=0.19->scikit-maad->-r requirements.txt (line 15)) (2.31.5)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-image>=0.19->scikit-maad->-r requirements.txt (line 15)) (2023.9.26)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-image>=0.19->scikit-maad->-r requirements.txt (line 15)) (0.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn>=0.19.1->librosa==0.9.1->-r requirements.txt (line 21)) (3.2.0)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<4.0.0,>=3.7.1 (from fastapi->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting httpcore==1.* (from httpx->gradio>=3.7.0->-r requirements.txt (line 4))\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpx->gradio>=3.7.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio>=3.7.0->-r requirements.txt (line 4)) (1.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.7.0->-r requirements.txt (line 4)) (0.10.6)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 22)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 22)) (3.2.2)\n",
      "Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading torchcrepe-0.0.22-py3-none-any.whl (72.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_maad-1.4.0-py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.5/152.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxsim-0.4.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading edge_tts-6.1.9-py3-none-any.whl (27 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading local_attention-1.9.0-py3-none-any.whl (8.2 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m980.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml-4.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: fairseq, pyworld, langdetect, antlr4-python3-runtime\n",
      "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=10389895 sha256=8d9671b46ab605e2d1d685d3478534ba53181f665f8e196499ce616a942a1fc1\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
      "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyworld: filename=pyworld-0.3.4-cp310-cp310-linux_x86_64.whl size=194740 sha256=86ff8419445810238f5f603e005e75b0c908e73ad98853ec9d3ffebffd0bf2c2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/66/09/8a/a1d79b73d59756f66e9bfe55a199840efc7473adb76ddacdfd\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=ee5fe7765411871938b62d13512b93419da52acd2ffb239b813a3ed252b989e8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141209 sha256=16d34f29388e6da236a53118272ec13776661a2c09f4cf2d77b3597f2c1ce7cc\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
      "Successfully built fairseq pyworld langdetect antlr4-python3-runtime\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: faiss-cpu, antlr4-python3-runtime, websockets, typing-extensions, triton, tomlkit, shellingham, semantic-version, safetensors, python-multipart, portalocker, orjson, numpy, lxml, loguru, langdetect, h11, ffmpeg-python, einops, certifi, audioread, anyio, annotated-types, aiofiles, uvicorn, typer, torch, tensorboardX, starlette, SoundFile, scipy, sacrebleu, pyworld, pydantic-core, praat-parselmouth, onnx, omegaconf, httpcore, torchaudio, resampy, pydantic, pooch, onnxsim, onnxoptimizer, local_attention, hydra-core, huggingface-hub, httpx, edge_tts, tokenizers, scikit-maad, librosa, gradio-client, fastapi, fairseq, altair, transformers, torchcrepe, gradio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.12.1\n",
      "    Uninstalling tomlkit-0.12.1:\n",
      "      Successfully uninstalled tomlkit-0.12.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.3\n",
      "    Uninstalling scipy-1.11.3:\n",
      "      Successfully uninstalled scipy-1.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mkl-fft 1.3.8 requires mkl, which is not installed.\n",
      "black 23.10.1 requires packaging>=22.0, but you have packaging 21.3 which is incompatible.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed SoundFile-0.12.1 aiofiles-23.2.1 altair-5.2.0 annotated-types-0.6.0 antlr4-python3-runtime-4.8 anyio-3.7.1 audioread-3.0.1 certifi-2023.7.22 edge_tts-6.1.9 einops-0.7.0 fairseq-0.12.2 faiss-cpu-1.7.4 fastapi-0.104.1 ffmpeg-python-0.2.0 gradio-4.7.1 gradio-client-0.7.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface-hub-0.19.4 hydra-core-1.0.7 langdetect-1.0.9 librosa-0.9.1 local_attention-1.9.0 loguru-0.7.2 lxml-4.9.3 numpy-1.23.5 omegaconf-2.0.6 onnx-1.15.0 onnxoptimizer-0.3.13 onnxsim-0.4.35 orjson-3.9.10 pooch-1.8.0 portalocker-2.8.2 praat-parselmouth-0.4.3 pydantic-2.5.2 pydantic-core-2.14.5 python-multipart-0.0.6 pyworld-0.3.4 resampy-0.4.2 sacrebleu-2.3.2 safetensors-0.4.1 scikit-maad-1.4.0 scipy-1.10.0 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.27.0 tensorboardX-2.6.2.2 tokenizers-0.15.0 tomlkit-0.12.0 torch-2.1.1+cu118 torchaudio-2.1.1+cu118 torchcrepe-0.0.22 transformers-4.35.2 triton-2.1.0 typer-0.9.0 typing-extensions-4.8.0 uvicorn-0.24.0.post1 websockets-11.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#@title Clone repository and install requirements\n",
    "\n",
    "#@markdown # Clone repository and install requirements\n",
    "\n",
    "#@markdown\n",
    "\n",
    "#@markdown ### After the execution is completed, the runtime will **automatically restart**\n",
    "\n",
    "#@markdown\n",
    "\n",
    "!git clone https://github.com/svc-develop-team/so-vits-svc -b 4.1-Stable\n",
    "\n",
    "# %cd /content/so-vits-svc\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "%pip install --upgrade pip setuptools\n",
    "%pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_PMPCN6wvgZ"
   },
   "outputs": [],
   "source": [
    "#@title Get pretrained model(Optional but strongly recommend).\n",
    "\n",
    "#@markdown # Get pretrained model(Optional but strongly recommend).\n",
    "\n",
    "#@markdown\n",
    "\n",
    "#@markdown - Pre-trained model files: `G_0.pth` `D_0.pth`\n",
    "#@markdown   - Place them under logs/44k/ \n",
    "\n",
    "#@markdown Get them from svc-develop-team(TBD) or anywhere else.\n",
    "\n",
    "#@markdown Although the pretrained model generally does not cause any copyright problems, please pay attention to it. For example, ask the author in advance, or the author has indicated the feasible use in the description clearly.\n",
    "\n",
    "download_pretrained_model = True #@param {type:\"boolean\"}\n",
    "D_0_URL = \"https://huggingface.co/datasets/ms903/sovits4.0-768vec-layer12/resolve/main/sovits_768l12_pre_large_320k/clean_D_320000.pth\" #@param [\"https://huggingface.co/datasets/ms903/sovits4.0-768vec-layer12/resolve/main/sovits_768l12_pre_large_320k/clean_D_320000.pth\", \"https://huggingface.co/1asbgdh/sovits4.0-volemb-vec768/resolve/main/clean_D_320000.pth\", \"https://huggingface.co/datasets/ms903/sovits4.0-768vec-layer12/resolve/main/vol_emb/clean_D_320000.pth\"] {allow-input: true}\n",
    "G_0_URL = \"https://huggingface.co/datasets/ms903/sovits4.0-768vec-layer12/resolve/main/sovits_768l12_pre_large_320k/clean_G_320000.pth\" #@param [\"https://huggingface.co/datasets/ms903/sovits4.0-768vec-layer12/resolve/main/sovits_768l12_pre_large_320k/clean_G_320000.pth\", \"https://huggingface.co/1asbgdh/sovits4.0-volemb-vec768/resolve/main/clean_G_320000.pth\", \"https://huggingface.co/datasets/ms903/sovits4.0-768vec-layer12/resolve/main/vol_emb/clean_G_320000.pth\"] {allow-input: true}\n",
    "\n",
    "download_pretrained_diffusion_model = True #@param {type:\"boolean\"}\n",
    "diff_model_URL = \"https://huggingface.co/datasets/ms903/Diff-SVC-refactor-pre-trained-model/resolve/main/fix_pitch_add_vctk_600k/model_0.pt\" #@param {type:\"string\"}\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "\n",
    "if download_pretrained_model:\n",
    "    !curl -L {D_0_URL} -o logs/44k/D_0.pth\n",
    "    !md5sum logs/44k/D_0.pth\n",
    "    !curl -L {G_0_URL} -o logs/44k/G_0.pth\n",
    "    !md5sum logs/44k/G_0.pth\n",
    "\n",
    "if download_pretrained_diffusion_model:\n",
    "    !mkdir -p logs/44k/diffusion\n",
    "    !curl -L {diff_model_URL} -o logs/44k/diffusion/model_0.pt\n",
    "    !md5sum logs/44k/diffusion/model_0.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1qadJBFehMo"
   },
   "source": [
    "# **Dataset preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBlju6Q3lSM6",
    "tags": []
   },
   "source": [
    "目的是把需要训练的声音放到(dataset_raw/)目录中\n",
    "\n",
    "文件结构如下: 这里一定要注意speaker/speaker0等是重要标识\n",
    "\n",
    "```\n",
    "YourforlderforSingleSpeakers\n",
    "└───speaker\n",
    "    ├───xxx1-xxx1.wav\n",
    "    ├───...\n",
    "    └───Lxx-0xx8.wav\n",
    "```\n",
    "\n",
    "```\n",
    "YourFolderforMultipleSpeakers\n",
    "├───speaker0\n",
    "│   ├───xxx1-xxx1.wav\n",
    "│   ├───...\n",
    "│   └───Lxx-0xx8.wav\n",
    "└───speaker1\n",
    "    ├───xx2-0xxx2.wav\n",
    "    ├───...\n",
    "    └───xxx7-xxx007.wav\n",
    "```\n",
    "\n",
    "**Even if there is only one speaker, a folder named `{speaker_name}` is needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆slide tools\n",
    "%cd /home/ec2-user/SageMaker\n",
    "%git clone https://github.com/openvpi/audio-slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#更新必要的音频处理相关的库\n",
    "%pip install numpy\n",
    "%pip install librosa\n",
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir /home/ec2-user/SageMaker/audio-slicer/audiowav\n",
    "\n",
    "#运行后将wav文件拷贝到audio-slicer/audiowav目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker/audio-slicer/\n",
    "SPEAKER_NAME = \"speak1\" #需要克隆的人的标志\n",
    "import os\n",
    "if not os.path.exists(\"/home/ec2-user/SageMaker/audio-slicer/dataset_raw\"):\n",
    "    %mkdir /home/ec2-user/SageMaker/audio-slicer/dataset_raw\n",
    "if not os.path.exists(\"/home/ec2-user/SageMaker/audio-slicer/dataset_raw/\"+SPEAKER_NAME):\n",
    "    %mkdir /home/ec2-user/SageMaker/audio-slicer/dataset_raw/{SPEAKER_NAME}\n",
    "\n",
    "\n",
    "import librosa  # Optional. Use any library you like to read audio files.\n",
    "import soundfile  # Optional. Use any library you like to write audio files.\n",
    "\n",
    "from slicer2 import Slicer\n",
    "\n",
    "audio, sr = librosa.load('audiowav/example.wav', sr=None, mono=False)  # Load an audio file with librosa. \n",
    "slicer = Slicer(\n",
    "    sr=sr,\n",
    "    threshold=-40,\n",
    "    min_length=5000,\n",
    "    min_interval=300,\n",
    "    hop_size=10,\n",
    "    max_sil_kept=500\n",
    ")\n",
    "chunks = slicer.slice(audio)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if len(chunk.shape) > 1:\n",
    "        chunk = chunk.T  # Swap axes if the audio is stereo.\n",
    "    soundfile.write(f'/home/ec2-user/SageMaker/audio-slicer/dataset_raw/{SPEAKER_NAME}/{i}{SPEAKER_NAME}_{i}.wav', chunk, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理的音频拷贝到训练目录下\n",
    "%cp -vr /home/ec2-user/SageMaker/audio-slicer/dataset_raw /home/ec2-user/SageMaker/so-vits-svc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ThKTzYs5CfL"
   },
   "outputs": [],
   "source": [
    "#@title Resample to 44100Hz and mono\n",
    "\n",
    "#@markdown # Resample to 44100Hz and mono\n",
    "\n",
    "#@markdown\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "!python resample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZThaMxmIJgWy"
   },
   "outputs": [],
   "source": [
    "#@title Train cluster model (Optional)\n",
    "\n",
    "#@markdown # Train cluster model (Optional)\n",
    "\n",
    "#@markdown #### Details see [README.md#cluster-based-timbre-leakage-control](https://github.com/svc-develop-team/so-vits-svc#cluster-based-timbre-leakage-control)\n",
    "\n",
    "#@markdown\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "!python cluster/train_cluster.py --gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svITReeL5N8K"
   },
   "outputs": [],
   "source": [
    "#@title Divide filelists and generate config.json\n",
    "\n",
    "#@markdown # Divide filelists and generate config.json\n",
    "\n",
    "#@markdown\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "%mkdir -p /home/ec2-user/SageMaker/so-vits-svc/dataset/44k\n",
    "\n",
    "speech_encoder = \"vec768l12\" #@param [\"vec768l12\", \"vec256l9\", \"hubertsoft\", \"whisper-ppg\", \"whisper-ppg-large\"]\n",
    "use_vol_aug = False #@param {type:\"boolean\"}\n",
    "vol_aug = \"--vol_aug\" if use_vol_aug else \"\"\n",
    "\n",
    "from pretrain.meta import download_dict\n",
    "download_dict = download_dict()\n",
    "\n",
    "url = download_dict[speech_encoder][\"url\"]\n",
    "output = download_dict[speech_encoder][\"output\"]\n",
    "\n",
    "import os\n",
    "if not os.path.exists(output):\n",
    "  !curl -L {url} -o {output}\n",
    "  !md5sum {output}\n",
    "\n",
    "!python preprocess_flist_config.py --speech_encoder={speech_encoder} {vol_aug}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENoH-pShel7w"
   },
   "source": [
    "# **Trainning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train index model (Optional)\n",
    "\n",
    "#@markdown # Train index model (Optional)\n",
    "\n",
    "#@markdown #### Details see [README.md#feature-retrieval](https://github.com/svc-develop-team/so-vits-svc#feature-retrieval)\n",
    "\n",
    "#@markdown\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "!python train_index.py -c configs/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHUXMi836DMe"
   },
   "outputs": [],
   "source": [
    "#@title Generate hubert and f0\n",
    "\n",
    "#@markdown # Generate hubert and f0\n",
    "\n",
    "#@markdown\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "\n",
    "f0_predictor = \"crepe\" #@param [\"crepe\", \"pm\", \"dio\", \"harvest\", \"rmvpe\", \"fcpe\"]\n",
    "use_diff = True #@param {type:\"boolean\"}\n",
    "\n",
    "import os\n",
    "if f0_predictor == \"rmvpe\" and not os.path.exists(\"./pretrain/rmvpe.pt\"):\n",
    "  !curl -L https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/rmvpe.pt -o pretrain/rmvpe.pt\n",
    "\n",
    "if f0_predictor == \"fcpe\" and not os.path.exists(\"./pretrain/fcpe.pt\"):\n",
    "  !curl -L https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/fcpe.pt -o pretrain/fcpe.pt\n",
    "\n",
    "\n",
    "diff_param = \"\"\n",
    "if use_diff:\n",
    "  diff_param = \"--use_diff\"\n",
    "\n",
    "  if not os.path.exists(\"./pretrain/nsf_hifigan/model\"):\n",
    "    !curl -L https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip -o nsf_hifigan_20221211.zip\n",
    "    !md5sum nsf_hifigan_20221211.zip\n",
    "    !unzip nsf_hifigan_20221211.zip\n",
    "    !rm -rf pretrain/nsf_hifigan\n",
    "    !mv -v nsf_hifigan pretrain\n",
    "\n",
    "!python preprocess_hubert_f0.py --f0_predictor={f0_predictor} {diff_param}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hEFFTCfZf57"
   },
   "outputs": [],
   "source": [
    "#@title Start training\n",
    "\n",
    "#@markdown # Start training\n",
    "\n",
    "#@markdown If you want to use pre-trained models, upload them to logs/44K下\n",
    "\n",
    "#@markdown\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "\n",
    "#@markdown Whether to enable tensorboard\n",
    "tensorboard_on = False  #@param {type:\"boolean\"}\n",
    "\n",
    "if tensorboard_on:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir logs/44k\n",
    "\n",
    "config_path = \"/home/ec2-user/SageMaker/so-vits-svc/configs/config.json\"\n",
    "\n",
    "from pretrain.meta import get_speech_encoder\n",
    "url, output = get_speech_encoder(config_path)\n",
    "\n",
    "import os\n",
    "if not os.path.exists(output):\n",
    "  !curl -L {url} -o {output}\n",
    "\n",
    "!python train.py -c {config_path} -m 44k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train diffusion model (Optional)\n",
    "\n",
    "#@markdown # Train diffusion model (Optional)\n",
    "\n",
    "#@markdown #### Details see [README.md#-about-shallow-diffusion](https://github.com/svc-develop-team/so-vits-svc#-about-shallow-diffusion)\n",
    "\n",
    "#@markdown\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"./pretrain/nsf_hifigan/model\"):\n",
    "  !curl -L https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip -o nsf_hifigan_20221211.zip\n",
    "  !unzip nsf_hifigan_20221211.zip\n",
    "  !rm -rf pretrain/nsf_hifigan\n",
    "  !mv -v nsf_hifigan pretrain\n",
    "\n",
    "#@markdown Whether to enable tensorboard\n",
    "tensorboard_on = False  #@param {type:\"boolean\"}\n",
    "\n",
    "if tensorboard_on:\n",
    "  %load_ext tensorboard\n",
    "  %tensorboard --logdir logs/44k\n",
    "\n",
    "!python train_diff.py -c configs/diffusion.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **多国语音**\n",
    "一共支持32种语言，具体使用那种语言请查阅\n",
    "\n",
    "https://docs.aws.amazon.com/polly/latest/dg/ntts-voices-main.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#使用polly可以产生36个国家语言，因为这里使用声纹克隆所以推荐的语音角色如下\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from contextlib import closing\n",
    "import tempfile\n",
    "import urllib3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "import decimal\n",
    "text='مرحبا بالجميع ، أنا مساعد صوت Amazon Web Services يمكنه قراءة معظم لغات ولهجات العالم ، ويمكنه محاكاة بصمة صوت شخص معين'\n",
    "languageCode = 'ar-AE' #语种\n",
    "voiceId = 'Hala' #性别角色\n",
    "speed_prefix = '<prosody rate=\"fast\">' #变速\n",
    "ssmltext='<speak>'+speed_prefix+text+'</prosody></speak>'\n",
    "c = boto3.client('polly',region_name='us-east-1')\n",
    "\n",
    "    \n",
    "try:\n",
    "    response = c.synthesize_speech(Engine='neural',\n",
    "                         LanguageCode=languageCode, \n",
    "                         OutputFormat='mp3',\n",
    "                         Text=ssmltext, \n",
    "                         TextType='ssml',\n",
    "                         VoiceId=voiceId)\n",
    "except (BotoCoreError, ClientError) as error:\n",
    "    # The service returned an error, exit gracefully\n",
    "    print(error)\n",
    "    sys.exit(-1)\n",
    "    \n",
    "if \"AudioStream\" in response:\n",
    "    # Note: Closing the stream is important because the service throttles on the\n",
    "    # number of parallel connections. Here we are using contextlib.closing to\n",
    "    # ensure the close method of the stream object will be called automatically\n",
    "    # at the end of the with statement's scope.\n",
    "        with closing(response[\"AudioStream\"]) as stream:\n",
    "           output = os.path.join(\"/home/ec2-user/SageMaker/so-vits-svc/raw\", \"speech.mp3\")\n",
    "           try:\n",
    "            # Open a file for writing the output as a binary stream\n",
    "                with open(output, \"wb\") as file:\n",
    "                   file.write(stream.read())\n",
    "           except IOError as error:\n",
    "              # Could not write to file, exit gracefully\n",
    "              print(error)\n",
    "              sys.exit(-1)\n",
    "\n",
    "else:\n",
    "    # The response didn't contain audio data, exit gracefully\n",
    "    print(\"Could not stream audio\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCnbX-OT897k",
    "tags": []
   },
   "source": [
    "# **Inference**\n",
    "### Upload wav files from this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此步只需成功执行一次\n",
    "#title Download nsf_hifigan if you need it\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "!curl -L https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip -o /home/ec2-user/SageMaker/so-vits-svc/nsf_hifigan_20221211.zip\n",
    "!unzip nsf_hifigan_20221211.zip\n",
    "!rm -rf pretrain/nsf_hifigan\n",
    "!mv -v nsf_hifigan pretrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#此步骤执行比较慢只需成功执行一次\n",
    "%conda install ffmpeg\n",
    "%conda install ffmpy\n",
    "%conda install ffprobe\n",
    "%conda install pydub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/so-vits-svc/raw\n"
     ]
    }
   ],
   "source": [
    "#如果初始声音为mp3则执行此步骤\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc/raw\n",
    "\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "\n",
    "# 读取mp3的波形数据\n",
    "sound = AudioSegment.from_file(\"/home/ec2-user/SageMaker/so-vits-svc/raw/speech.mp3\", format = 'MP3')\n",
    "\n",
    "f = wave.open(\"YourWAVFile.wav\", 'wb')\n",
    "f.setnchannels(1)   # 频道数\n",
    "f.setsampwidth(2)   # 量化位数\n",
    "f.setframerate(16000)   # 取样频率\n",
    "f.setnframes(len(sound._data))   # 取样点数，波形数据的长度\n",
    "f.writeframes(sound._data)   # 写入波形数据\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dYnKuKTIj3z1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/so-vits-svc\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "load \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "load model(s) from pretrain/checkpoint_best_legacy_500.pt\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "#=====segment start, 10.8s======\n",
      "/home/ec2-user/SageMaker/so-vits-svc/inference/infer_tool.py:270: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "vits use time:0.8020908832550049\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title Start inference (and download)\n",
    "\n",
    "#@markdown # Start inference (and download)\n",
    "\n",
    "#@markdown Parameters see [README.MD#Inference](https://github.com/svc-develop-team/so-vits-svc#-inference)\n",
    "\n",
    "#@markdown\n",
    "\n",
    "wav_filename = \"YourWAVFile.wav\"  #@param {type:\"string\"}\n",
    "model_filename = \"G_800.pth\"  #@param {type:\"string\"}\n",
    "model_path = \"/home/ec2-user/SageMaker/so-vits-svc/logs/44k/\" + model_filename\n",
    "speaker = \"speak1\"  #@param {type:\"string\"}\n",
    "trans = \"0\"  #@param {type:\"string\"}\n",
    "cluster_infer_ratio = \"0\"  #@param {type:\"string\"}\n",
    "auto_predict_f0 = True  #@param {type:\"boolean\"}\n",
    "apf = \"\"\n",
    "if auto_predict_f0:\n",
    "  apf = \" -a \"\n",
    "\n",
    "f0_predictor = \"crepe\" #@param [\"crepe\", \"pm\", \"dio\", \"harvest\", \"rmvpe\", \"fcpe\"]\n",
    "\n",
    "enhance = False  #@param {type:\"boolean\"}\n",
    "ehc = \"\"\n",
    "if enhance:\n",
    "  ehc = \" -eh \"\n",
    "#@markdown\n",
    "\n",
    "#@markdown Generally keep default:\n",
    "config_filename = \"config.json\"  #@param {type:\"string\"}\n",
    "config_path = \"/home/ec2-user/SageMaker/so-vits-svc/configs/\" + config_filename\n",
    "\n",
    "# from pretrain.meta import get_speech_encoder\n",
    "# url, output = get_speech_encoder(config_path)\n",
    "\n",
    "import os\n",
    "\n",
    "if f0_predictor == \"rmvpe\" and not os.path.exists(\"/home/ec2-user/SageMaker/so-vits-svc/pretrain/rmvpe.pt\"):\n",
    "  !curl -L https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/rmvpe.pt -o pretrain/rmvpe.pt\n",
    "\n",
    "if f0_predictor == \"fcpe\" and not os.path.exists(\"/home/ec2-user/SageMaker/so-vits-svc/pretrain/fcpe.pt\"):\n",
    "  !curl -L https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/fcpe.pt -o pretrain/fcpe.pt\n",
    "\n",
    "# if not os.path.exists(output):\n",
    "#   !curl -L {url} -o {output}\n",
    "\n",
    "kmeans_filenname = \"kmeans_10000.pt\"  #@param {type:\"string\"}\n",
    "kmeans_path = \"/home/ec2-user/SageMaker/so-vits-svc/logs/44k/\" + kmeans_filenname\n",
    "slice_db = \"-40\"  #@param {type:\"string\"}\n",
    "wav_format = \"wav\"  #@param {type:\"string\"}\n",
    "\n",
    "key = \"auto\" if auto_predict_f0 else f\"{trans}key\"\n",
    "cluster_name = \"\" if cluster_infer_ratio == \"0\" else f\"_{cluster_infer_ratio}\"\n",
    "isdiffusion = \"sovits\"\n",
    "wav_output = f\"/home/ec2-user/SageMaker/so-vits-svc/results/{wav_filename}_{key}_{speaker}{cluster_name}_{isdiffusion}_{f0_predictor}.{wav_format}\"\n",
    "\n",
    "%cd /home/ec2-user/SageMaker/so-vits-svc\n",
    "!python inference_main.py -n {wav_filename} -m {model_path} -s {speaker} -t {trans} -cr {cluster_infer_ratio} -c {config_path} -cm {kmeans_path} -sd {slice_db} -wf {wav_format} {apf} --f0_predictor={f0_predictor} {ehc}\n",
    "\n",
    "#@markdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "19fxpo-ZoL_ShEUeZIZi6Di-YioWrEyhR",
     "timestamp": 1678516497580
    },
    {
     "file_id": "1rCUOOVG7-XQlVZuWRAj5IpGrMM8t07pE",
     "timestamp": 1673086970071
    },
    {
     "file_id": "1Ul5SmzWiSHBj0MaKA0B682C-RZKOycwF",
     "timestamp": 1670483515921
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
